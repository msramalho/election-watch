{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding structure through mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# required imports to access api_db, misc, misc.CONFIG, ...\n",
    "import sys\n",
    "sys.path = ['.', '..', '../..'] + sys.path\n",
    "from collection import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<h1 align=\"center\">driver code</h1>\n",
    "\n",
    "1. Extract data from db -> organize in local file\n",
    "2. Train embeddings\n",
    "3. Export embeddings (maybe model)\n",
    "4. Upload for tensor viewer\n",
    "5. delete local file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER = \"../embeddings/\"\n",
    "NAME = \"tweet_relations_mentions\"\n",
    "MODEL = abs_path(FOLDER + NAME + \".model\")\n",
    "TF_OUT = abs_path(FOLDER + NAME + \"_tf_out\")\n",
    "# JOBLIB = abs_path(FOLDER + NAME + \".joblib\")\n",
    "# CSV = abs_path(\"embeddings/\" + NAME + \".csv\") # done dynamically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER_FULL_PATH = abs_path(FOLDER)\n",
    "if not os.path.exists(FOLDER_FULL_PATH): os.makedirs(FOLDER_FULL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_options = {'user_mentions.1': {\"$exists\": True}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "total = api_db.col_tweets.count_documents(filter_options)\n",
    "print(\"Total to process: %d\" % total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_mentions(skip, limit):\n",
    "    filter_options = {'user_mentions.1': {\"$exists\": True}}\n",
    "    tweets = api_db.col_tweets.find(filter_options, {\"user\": True, \"user_mentions\": True}).skip(skip).limit(limit)\n",
    "    pairs = []\n",
    "    step=1e4\n",
    "    for t in tweets:\n",
    "        try:\n",
    "            pairs.append(t[\"user_mentions\"]) # accumulate\n",
    "            # print in batches for speed\n",
    "            if len(pairs)>=step:\n",
    "                for ps in pairs: print(\",\".join(map(str, ps)))\n",
    "                pairs = []\n",
    "        except: continue        \n",
    "    for ps in pairs: print(\",\".join(map(str, ps)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp = DynamicParallelism(total, task_mentions, NAME, batch_size=total//4, max_threads=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CSV = dp.run().reduce()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp.clean()"
   ]
  },
  {
   "source": [
    "---\n",
    "# train embeddings"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelationsTweetsCorpus:\n",
    "    \"\"\"An interator that yields sentences (lists of str).\"\"\"\n",
    "    def __init__(self): pass\n",
    "    def __iter__(self):\n",
    "        step = 5e5\n",
    "        with DoneMessage(\"Iterating mentions corpus\"):\n",
    "            for line in open(CSV, encoding=\"utf-8\"):\n",
    "                # assume there's one document per line, tokens separated by comma\n",
    "                yield list(map(str,line.strip().split(\",\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EMBEDDINGS CONFIGS\n",
    "MIN_COUNT = 25\n",
    "SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(sentences=RelationsTweetsCorpus(), compute_loss=True, min_count=MIN_COUNT, size=SIZE, window=1000) # default alpha=0.025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check https://medium.com/@aakashchotrani/visualizing-your-own-word-embeddings-using-tensorflow-688b3a7750ee\n",
    "# to use with `python -m gensim.scripts.word2vec2tensor -i INPUT_FILE_PATH -o OUTPUT_FILE_PATH`\n",
    "with DoneMessage(\"Saving model locally\"):\n",
    "    model.wv.save_word2vec_format(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with DoneMessage(\"Output embeddings for visualization\"):\n",
    "    os.system(\"python -m gensim.scripts.word2vec2tensor -i %s -o %s\" % (MODEL, TF_OUT))"
   ]
  },
  {
   "source": [
    "---\n",
    "# converting _ids to screen_names"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "METADATA_IDS = TF_OUT + \"_metadata.tsv\"\n",
    "METADATA_HANDLES = TF_OUT + \"_metadata_handles.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CACHE_FILE = abs_path(\"../_cache_id_screen_name.json\")\n",
    "CACHE = {}\n",
    "try:\n",
    "    if os.path.isfile(CACHE_FILE): CACHE = json_to_dict(CACHE_FILE)\n",
    "except: print(\"Failed to load cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "counter = 0\n",
    "with DoneMessage(\"Converting _ids to handles\"):\n",
    "    with open(METADATA_IDS, \"r\", encoding=\"utf-8\") as inf:\n",
    "        with open(METADATA_HANDLES, \"w\", encoding=\"utf-8\") as outf:\n",
    "            for _id in inf:\n",
    "                _id = _id.strip()\n",
    "                # check cache\n",
    "                if _id in CACHE:\n",
    "                    outf.write(\"%s\\n\" % CACHE[_id])\n",
    "                    continue\n",
    "\n",
    "                # check if in db\n",
    "                account = api_db.col_users.find_one({\"_id\": int(_id), \"screen_name\": {\"$exists\": True}}, {\"screen_name\": True})\n",
    "                # query api\n",
    "                if not account: \n",
    "                    temp_account = get_account_details(user_id=_id)\n",
    "                    if temp_account: account = user_to_db_format(temp_account)\n",
    "                # save either found screen_name or default _id\n",
    "                if account:\n",
    "                    CACHE[_id] = account[\"screen_name\"]\n",
    "                    outf.write(\"%s\\n\" % account[\"screen_name\"])\n",
    "                    counter+=1\n",
    "                else: outf.write(\"%s\\n\" % _id) # defaults to the _id value\n",
    "                if counter%10 == 0: dict_to_json(CACHE, CACHE_FILE)\n",
    "\n",
    "dict_to_json(CACHE, CACHE_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# remove the MODEL\n",
    "with DoneMessage(\"removing the model\"):\n",
    "    try: os.remove(MODEL)\n",
    "    except: print(\"failed to remove the model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"DONE\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}